Domain Generalization Method Based on CLIP and Center Loss for Driver Distraction Detection
====

The discrepancy between the actual application scenarios and the training data collection scenarios leads to the domain shift between the training and test data, which restricts the performance of existing driver distraction detection methods in practical applications. To address the domain shift problem and promote the deployment of deep learning models in practical scenarios, in this paper, we propose a domain generalization method based on the contrastive language-image pretraining (CLIP) model and the constraint of center loss (DGCCL) for driver distraction detection. Firstly, the image encoder of the pre-trained CLIP model is adopted as the feature extraction module to improve the domain generalization ability of the proposed model. Furthermore, the constraint of center loss is introduced to promote the samples of different datasets to follow an approximately identical distribution in the feature space, thereby alleviating the domain shift problem. Additionally, the classification loss with additive angular margin penalty (AAMP) is introduced in the proposed model to further improve the cross-domain performance. In order to demonstrate the effectiveness of the proposed method, extensive experiments have been conducted on three publicly available driver distraction detection datasets: AUC-DDD, State-Farm, and SAM-DD. The experimental results verify that our method can achieve much better performance than various well-known image classification models in the cross-domain driver distraction detection tasks.
